{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"TF-IDF\"\n",
    "subtitle: \"with a recap of material covered so far\"\n",
    "author:\n",
    "  - name: Charles Pletcher\n",
    "    affiliations: Tufts University\n",
    "    orcid: 0000-0003-2734-5511\n",
    "    email: charles.pletcher@tufts.edu\n",
    "license:\n",
    "  code: MIT\n",
    "date: 2025-03-11\n",
    "---\n",
    "\n",
    "## TF-IDF - An Introduction\n",
    "\n",
    "Term Frequency — Inverse Document Frequency, or TF-IDF, is a powerful, if deceptively simple, way of determining keywords in a text. As its name suggests, it is comprised of two parts:\n",
    "\n",
    "### Term Frequency (TF)\n",
    "\n",
    "The number of times that a term — typically, a token or a phrase — appears in a document. We can normalize this using various techniques. Generally speaking, we want to use the _relative_ frequency, or the raw frequency divided by the number of terms in the document.\n",
    "\n",
    "### Document Frequency (DF)\n",
    "\n",
    "The number of documents in which a term appears. Again, there are common normalizations that we will typically perform, such as adding a small ε value to avoid dividing by zero.\n",
    "\n",
    "_Inverse_ document frequency, or IDF, is just the multiplicative inverse, or reciprocal, of DF.\n",
    "\n",
    "In practice, however, IDF is usually logarithmically scaled, i.e., $IDF=\\log(n/DF)$ where $n$ is the number of documents in the corpus.\n",
    "\n",
    "## Prefatory matters\n",
    "\n",
    "In order to get a sense of a basic implementation of TF-IDF, we first need some text to work with. To get some text, we'll need to review a few things about Python, and we'll need to learn how to use the `lxml` library to get text out of TEI XML files — the kinds of files that store much of the textual data and metadata that we use in digital humanities.\n",
    "\n",
    "### Install `lxml`\n",
    "\n",
    "`%pip` (note the `%`) is a special command built into Jupyter notebooks that lets us use the Python package manager `pip`.\n",
    "\n",
    "To install a package, we just enter `%pip install` and then the package name in a code cell, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "\n",
    "With `lxml` installed, we now need to import the `etree` library, along with the `Path` constructor from the built-in `pathlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize our path\n",
    "\n",
    "We'll use the `Path` constructor to grab the XML files that we need to parse:\n",
    "\n",
    ":::{tip}\n",
    "You can read more about **glob patterns** [here](https://code.visualstudio.com/docs/editor/glob-patterns).\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = Path(\"./xml/tlg0012\").glob(\"**/*perseus-eng*.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up namespaces for TEI parsing\n",
    "\n",
    "We also need to define a few namespaces to make it easier to find what we need in the XML files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEI_NS = \"http://www.tei-c.org/ns/1.0\"\n",
    "XML_NS = \"http://www.w3.org/XML/1998/namespace\"\n",
    "\n",
    "NAMESPACES = {\n",
    "    \"tei\": TEI_NS,\n",
    "    \"xml\": XML_NS,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the files\n",
    "\n",
    "Now we're ready to iterate through the files and extract the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    # print the name of the file as a sanity check\n",
    "    print(file)\n",
    "    \n",
    "    # etree.parse() reads the file and turns the raw XML into an object that we can use in Python\n",
    "    tree = etree.parse(file)\n",
    "\n",
    "    # xpath() is a method that applies **xpath expressions** to search through the XML.\n",
    "    # This xpath expression says, \"Find any `tei:div` element with a `subtype` of `'card'`.\n",
    "    # Under that element, get any text.\" The second argument, `namespaces=`, tells the\n",
    "    # method to use the supplied namespaces as shortcuts, so we don't have to type out\n",
    "    # \"http://www.tei-c.org/ns/1.0\" every time we want an element in the TEI namespace.\n",
    "\n",
    "    text = tree.xpath(f\"//tei:div[@subtype='card']//text()\", namespaces=NAMESPACES)\n",
    "\n",
    "    # xpath() returns an array of matches, so we initialize an empty array to store the\n",
    "    # results. We could use a list comprehension, but for now rewriting these\n",
    "    # lines as a list comprehension is left as an exercise for the reader.\n",
    "    cleaned_text = []\n",
    "\n",
    "    # Now we iterate through each string returned by `xpath()`\n",
    "    for t in text:\n",
    "        # `strip()` removes leading and trailing whitespace; if all that's left is an empty\n",
    "        # string, we don't care about it.\n",
    "        if t.strip() != \"\":\n",
    "            cleaned_text.append(t.strip())\n",
    "\n",
    "    # We make sure that we actually *have* text before writing just the text, without\n",
    "    # TEI elements, to a separate file. No need to write an empty file, right?\n",
    "    if len(cleaned_text) > 0:\n",
    "        # A lot is happening here:\n",
    "        #\n",
    "        # 1. `str(file)` turns the `Path` object into a `str`\n",
    "        # 2. `split(\"/\")` splits the resulting string at every \"/\"\n",
    "        # 3. `[-1]` takes the last element of the list returned by `split(\"/\")`\n",
    "        # 4. `replace(\".xml\", \".txt\")` changes the extension of the file\n",
    "        # \n",
    "        # So something like \"xml/tlg0012/tlg001/tlg0012.tlg001.perseus-eng3.xml\"\n",
    "        # is transformed into \"tlg0012.tlg001.perseus-eng3.txt\".\n",
    "        with open(str(file).split(\"/\")[-1].replace(\".xml\", \".txt\"), \"w+\") as f:\n",
    "            # We write the text to the file, `join`-ing each element in\n",
    "            # `cleaned_text` with a newline (\"\\n\")\n",
    "            f.write(\"\\n\".join(cleaned_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF in Action\n",
    "\n",
    "Now that we have our text files, we can start implementing some basic TF-IDF analysis.\n",
    "\n",
    "First, let's tokenize our text. In the past, we have used `split()` and regular expressions to handle tokenization, but we need to start getting serious about handling punctuation. Let's use the Natural Language Toolkit, or NLTK, instead.\n",
    "\n",
    "### Install the NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download support files\n",
    "\n",
    "With the NLTK installed, we can now download the support files that it needs for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# download the files needed for tokenization\n",
    "# the punkt tokenizer should be installed already,\n",
    "# but let's download it just in case\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize texts\n",
    "\n",
    "With the tokenizer downloaded, we can now read in and tokenize each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Initialize an empty dictionary to store the tokenized texts\n",
    "tokenized_texts = {}\n",
    "\n",
    "# Get a Path.glob() iterator for the .txt files that you've created in this directory.\n",
    "# Can you figure out what the new `[1-4]` segment is doing?\n",
    "text_files = Path(\".\").glob(\"tlg0012.tlg00*.perseus-eng[1-4].txt\")\n",
    "\n",
    "# Iterate through the text files, reading and tokenizing them one by one,\n",
    "# then storing the list of tokens in our `tokenized_texts` dictionary —\n",
    "# so we'll be getting a dictionary of lists.\n",
    "for file in text_files:\n",
    "    name = str(file)\n",
    "\n",
    "    with open(file) as f:\n",
    "        # Notice we're lowercasing the text. You don't *have*\n",
    "        # to do this, but it helps eliminate some noise for\n",
    "        # our purposes.\n",
    "        text = f.read().lower()\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Let's just print the length of the tokens list to make\n",
    "        # sure we're getting sane results. We'll use string interpolation\n",
    "        # to identify which text we're working with.\n",
    "        print(f\"There are {len(tokens)} tokens in {name}.\")\n",
    "\n",
    "        # Store each file's `tokens` list in the `tokenized_texts`\n",
    "        # dictionary, using the filename as the key.\n",
    "        tokenized_texts[name] = tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the tokens\n",
    "\n",
    "Now, we could count these tokens by hand, but why do that when Python gives us the `Counter` object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Using our `tokenized_texts` dictionary, we'll iterate\n",
    "# through each key-value pair — remember, the keys are\n",
    "# filenames and the values are lists of tokens.\n",
    "# We'll get a count of the tokens by passing the list to\n",
    "# `Counter`, then we'll change the value for that key to\n",
    "# a dictionary with its own keys, `tokens` and `counts`.\n",
    "\n",
    "for filename, tokens in tokenized_texts.items():\n",
    "    counts = Counter(tokens)\n",
    "\n",
    "    tokenized_texts[filename] = {\"tokens\": tokens, \"counts\": counts}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check to see what our frequencies look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts[\"tlg0012.tlg001.perseus-eng3.txt\"][\"counts\"][\"odysseus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the document frequency for a given term\n",
    "\n",
    "Let's compare occurrences of the strings `\"odysseus\"` and `\"achilles\"` — we probably\n",
    "expect the former to \"matter\" more for the _Odysseus_, and the latter for the _Iliad_. Let's see if that's the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_achilles = 0\n",
    "df_odysseus = 0\n",
    "\n",
    "# Calculate the DF for \"odysseus\" and \"achilles\".\n",
    "# We iterate through the dictionary, and then simply\n",
    "# count the number of files in which we find each term.\n",
    "# For these two terms, we should probably expect DFs of 4.\n",
    "for filename, values in tokenized_texts.items():\n",
    "    if \"odysseus\" in values['counts']:\n",
    "        df_odysseus += 1\n",
    "    \n",
    "    if \"achilles\" in values[\"counts\"]:\n",
    "        df_achilles += 1\n",
    "\n",
    "# Now we'll import the log function to calculate the IDF for each term.\n",
    "from math import log10\n",
    "\n",
    "n_docs = len(tokenized_texts.keys())\n",
    "\n",
    "idf_achilles = log10(n_docs / df_achilles)\n",
    "idf_odysseus = log10(n_docs / df_odysseus)\n",
    "\n",
    "print(idf_achilles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's calculate the TF-IDF \"score\" for each term in each document.\n",
    "\n",
    "# Once again, iterate through the dictionary.\n",
    "for filename, values in tokenized_texts.items():\n",
    "    # Get the total number of terms in each file — we'll\n",
    "    # use this to calculate the relative frequency as our\n",
    "    # TF.\n",
    "    total_terms = len(values['tokens'])\n",
    "\n",
    "    # Get the TF for each term in this file.\n",
    "    tf_achilles = values['counts']['achilles'] / total_terms\n",
    "    tf_odysseus = values['counts']['odysseus'] / total_terms\n",
    "\n",
    "    # Remember, the simplest version of TF-IDF is just\n",
    "    # TF * 1/DF\n",
    "    tf_idf_achilles = tf_achilles * idf_achilles\n",
    "    tf_idf_odysseus = tf_odysseus * idf_odysseus\n",
    "\n",
    "    # Now we can report on the statistics for this file\n",
    "    print(f\"\"\"In {filename}:\n",
    "TF of achilles: {tf_achilles}\n",
    "TF of odysseus: {tf_odysseus}\n",
    "TF-IDF of achilles: {tf_idf_achilles}\n",
    "TF-IDF of odysseus: {tf_idf_odysseus}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the TF-IDF isn't super interesting, but at least we have the TF to fall back on.\n",
    "\n",
    ":::{note}\n",
    "Can you describe what happened? How might we solve this problem?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overcoming frequent words\n",
    "\n",
    "In such a small corpus, it can be difficult to guess words that don't appear in all documents — certainly names of heroes like Achilles and Odysseus will appear in all of the documents, meaning $\\log(n/DF)$ will always be 0.\n",
    "\n",
    "But we can use the `set()` constructor to find words that do not appear in all of the documents, and calculate TF-IDF on them.\n",
    "\n",
    "In programming, a **set** is very similar to a list, except it guarantees that it only contains at most 1 of every element.\n",
    "\n",
    "Let's start small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [1, 1, 2, 3, 3]\n",
    "\n",
    "set(my_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how calling `set()` on `my_list` gets rid of the duplicate 1s and 3s. We can do likewise with strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_universal_terms = {}\n",
    "\n",
    "for filename, values in tokenized_texts.items():\n",
    "    my_set = set(values['counts'].keys())\n",
    "\n",
    "    for other_file, other_values in tokenized_texts.items():\n",
    "        # make sure we don't compare the file\n",
    "        # to itself, otherwise the difference\n",
    "        # will be the empty set\n",
    "        if other_file != filename:\n",
    "            my_set -= set(other_values['counts'].keys())\n",
    "    \n",
    "    # now push the remaining set of terms to the dictionary\n",
    "    non_universal_terms[filename] = my_set\n",
    "\n",
    "# log `non_universal_terms` as a sanity check\n",
    "non_universal_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "\n",
    "Now that you've seen the basics of TF-IDF, use the `set`s that we've built to explore the values for several terms in this corpus.\n",
    "\n",
    "As you explore, make note of any findings that seem odd — are they errors in how we have run the analysis, or is the text just weird?\n",
    "\n",
    "Consider, too, how you might improve the analysis in the future.\n",
    "\n",
    "Then in your own words, describe _what_ TF-IDF is really telling us about a given term in each text and within the corpus as a whole.\n",
    "\n",
    "Finally, think about other objects of study for which TF-IDF might be useful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
